{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLBD - \n",
    "\n",
    "## Lab - Feature Selection -\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 1 - Noisy Iris -\n",
    "\n",
    "- Developed by _Gary Marigliano - July 2018_\n",
    "\n",
    "- Modified by _Shabnam Ataee - March 2020_\n",
    "\n",
    "## Assistant -\n",
    "Shabnam Ataee\n",
    "\n",
    "## Introduction -\n",
    "\n",
    "First, read _ReadMe_ notebook and install required packages for this lab.\n",
    "\n",
    "In this exercise, the [famous iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset has been modified to insert noisy features. The goal is to retrieve 4 original features (sepal length/width and petal length/width) using feature selection models.\n",
    "\n",
    "You can use some feature selection algorithms listed here (the python library should already be installed for this exercise): http://featureselection.asu.edu/html/skfeature.function.html and http://featureselection.asu.edu/tutorial.php\n",
    "\n",
    "## ToDo in this notebook -\n",
    "\n",
    "Answer to questions in this notebook where **_ToDo_** is written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group members -\n",
    "\n",
    "As mentioned in the _ReadMe_ notebook, during this lab you will work in groups composed of 2 or 3 students. Please specify firstname, lastname and email address of group members here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ToDo ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Iris dataset -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the dataset is modified to create new noisy features to the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.1          3.5          1.4          0.2         12.           3.27265586\n",
      "    3.01640062   3.55403767   1.60548721   4.47214325   5.82137418\n",
      "    2.38411585   4.07326902   0.13133793   6.97527672   2.34652693\n",
      "    6.35289761   4.53652924   3.7418375    4.55191418   1.42010491\n",
      "   -0.38458357]\n",
      " [  4.9          3.           1.4          0.2         12.           2.77265586\n",
      "    2.51640062   3.05403767   1.10548721   3.97214325   5.32137418\n",
      "    1.88411585   3.57326902  -0.36866207   6.47527672   1.84652693\n",
      "    5.85289761   4.03652924   3.2418375    4.05191418   0.92010491\n",
      "   -0.88458357]\n",
      " [  4.7          3.2          1.3          0.2         12.           2.97265586\n",
      "    2.71640062   3.25403767   1.30548721   4.17214325   5.52137418\n",
      "    2.08411585   3.77326902  -0.16866207   6.67527672   2.04652693\n",
      "    6.05289761   4.23652924   3.4418375    4.25191418   1.12010491\n",
      "   -0.68458357]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "## Add some noisy features in the iris dataset\n",
    "\n",
    "# Add a feature that is always equal to a constant independently of the output --> useless feature\n",
    "constant_features = np.array([[12 for _ in range(X.shape[0])]]).transpose()\n",
    "X = np.append(X, constant_features, axis=1)\n",
    "\n",
    "# Add random noisy features. \n",
    "# These features are created using the first feature values with a more or less important noise level\n",
    "noise_levels = np.arange(1, 6, 0.3)\n",
    "first_feat = X[:, 1]\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "for k in noise_levels:\n",
    "    noise = k*(np.random.rand() * 2 - 1)\n",
    "    noisy_features = [noise + first_feat[x] for x in range(n_samples)]\n",
    "    noisy_features = np.array([noisy_features]).transpose()\n",
    "    X = np.append(X, noisy_features, axis=1)\n",
    "\n",
    "# Here we can see that the 5th column is always equals to 12. The colunms after it are the noisy features.\n",
    "print(X[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start selecting relevant features. To do that, we do \"Data Preparation\" phase of data science pipeline.\n",
    "\n",
    "This means we need to split the data into a train set (67%) and a test set (33%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, the example below shows how to train and get the features sorted by decreasing importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# train\n",
    "clf = ExtraTreesClassifier(n_jobs=2, n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the score\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score {:.3f}\".format(score))\n",
    "\n",
    "# rank the features\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# get the features sorted by decreasing importance\n",
    "feat_importances_sorted = [(indices[f], importances[indices[f]]) for f in range(n_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_ToDo_**: \n",
    "* Draw feature importance plot using a bar chart (see picture below)\n",
    "* Answer the following questions:\n",
    "   * What does this plot represent?\n",
    "   * How do you compare two features using this plot?\n",
    "   * How would you choose a \"good\" number of features?\n",
    "   * How can you be sure that the features you have been selected are relevant? What kind of tasks should you do?\n",
    "   * How could you prove it?\n",
    "   * For this modified dataset, is it really useful to reduce the number of features?\n",
    "   * How easy/hard is it to retrieve the original features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/01-noisy-iris-feat-importances-example.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ToDo ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best _n_ features -\n",
    "\n",
    "Now that we have the features sorted by decreasing importances, your task is to choose the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function plots the confusion matrix.\n",
    "# You can use this function to plot confusion matrix.\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title, cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_ToDo_**:\n",
    "\n",
    "* Choose _n_ features that you find relevant.\n",
    "* Justify the number _n_ that you have chosen.\n",
    "* Select confusion matrix or another relevant score metric and compare the classifier performance between:\n",
    "    * your selected features and the noisy iris dataset \n",
    "    * your selected features and some _n_ random features (take the average score of K runs for random features)\n",
    "    * your selected features and the worst _n_ features (look at your feature importance plot)\n",
    "* Answer the following questions:\n",
    "    * Among the features you have selected, how many are the original ones?\n",
    "    * Among the features you have selected, is there any useless feature (the one which always contains the same value)?\n",
    "    \n",
    "To plot a prettier confusion matrix you can use the follwoing code:\n",
    "\n",
    "``` python\n",
    "y_pred = clf.predict(X_test_random)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "n_classes = len(np.unique(y))\n",
    "plot_confusion_matrix(cm, classes=range(n_classes), title=\"Confusion Matrix\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ToDo ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further (optional) -\n",
    "\n",
    "Now that you finished this notebook, it can be interesting to go one step further and try following scenarios:\n",
    "\n",
    "* Can we achieve better results (i.e. more relevant features and/or less features) if we normalize the data set?\n",
    "* Can we retrieve the same relevant features by applying another feature selection model?\n",
    "* Plot the classifier performance for the best K features where K is $1, 2,..,k_{-1},k$ and comment the results.\n",
    "* ...\n",
    "\n",
    "Please answer to the above questions below in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
